# Raspador de Dados SUAPE â€“ 'DataÂ Scraper & WebÂ Harvester'

> **Projeto original:** Desafio SUAPE (InovaÃ§Ã£o Abertaâ€¯2025)
> **Arquivo principal:** `extracao.py`
> **Autor:** Mayvonâ€¯Alves
> **LicenÃ§a:** MIT

Este script surgiu para **contar quantas empresas existem no Complexo de SUAPE**. Depois, foi transformado numa pequena **ferramenta de raspagem (scraping) genÃ©rica**, que vocÃª pode usar em qualquer site que tenha uma lista repetitiva (cartÃµes, tabelas, blocos etc.).

Se vocÃª nunca raspou dados antes, siga este passoâ€‘aâ€‘passo.

---

## 1. O que o script faz?

1. **Acessa** a pÃ¡gina indicada (online ou salva em disco).
2. **Encontra** cada bloco de empresa (ou produto, notÃ­cia, eventoâ€¦).
3. **LÃª** as informaÃ§Ãµes principais (nome, atividade, latitude, longitude).
4. **Salva** tudo num arquivo **GeoJSON** (`empresas_suape.geojson`). Esse formato funciona em QGIS, GeoPandas, GoogleÂ Maps, etc.

---

## 2. InstalaÃ§Ã£o RÃ¡pida

```bash
# 1) Instale Python 3.9+ se ainda nÃ£o tiver.
# 2) Baixe o projeto
git clone https://github.com/seu-usuario/suape-data-scraper.git
cd suape-data-scraper

# 3) Instale as bibliotecas necessÃ¡rias
pip install -r requirements.txt  # leva menos de 1 minuto
```

> **Dica:** Se nÃ£o for usar Selenium, vocÃª pode remover essa linha do `requirements.txt` para instalar menos coisas.

---

## 3. Primeiro teste (sem alterar nada)

```bash
python extracao.py  # usa o endereÃ§o padrÃ£o de SUAPE
```

Depois de alguns segundos vocÃª verÃ¡ algo como:

```
[INFO] 83 empresas encontradas
[INFO] Arquivo 'empresas_suape.geojson' salvo
```

Pronto! O arquivo aparece na mesma pasta.

---

## 4. Usar em QUALQUER outro site

O script precisa de um **arquivo de configuraÃ§Ã£o** dizendo onde estÃ£o os dados na pÃ¡gina. Exemplo bem curto:

**site\_config.json**

```json
{
  "empresa_block": "div.card",
  "nome": ".card-title",
  "atividade": ".card-atividade",
  "lat": "data-lat",
  "lng": "data-lng"
}
```

* `empresa_block` â†’ o CSS do bloco que se repete.
* `nome`, `atividade` â†’ onde pegar texto.
* `lat`, `lng` â†’ atributo ou seletor com as coordenadas.
* Campos que nÃ£o existir (ex.: `polo`) podem ser omitidos.

### Executando

```bash
CONFIG_FILE=site_config.json \
python extracao.py --url "https://meu-site.com/lista"
```

Em segundos vocÃª terÃ¡ `meu-site.geojson` com os pontos.

### E se o site nÃ£o mostrar latitude/longitude?

VocÃª ainda pode usar o script, mas terÃ¡ que converter endereÃ§os em coordenadas (chamaâ€‘se **geocodificaÃ§Ã£o**). Isso nÃ£o estÃ¡ incluÃ­do aqui, mas serviÃ§os como Nominatim ou GoogleÂ Maps API fazem isso.

---

## 5. Funciona mesmo SEM internet

Alguns ambientes (por exemplo, notebooks online) bloqueiam acesso Ã  web. FaÃ§a assim:

1. Abra a pÃ¡gina no seu navegador.
2. Salve como **HTML Completo** (Ctrl+S) e renomeie para `pagina.html`.
3. Execute:

   ```bash
   HTML_FALLBACK=pagina.html python extracao.py
   ```

O script lÃª o arquivo local.

---

## 6. OpÃ§Ãµes Ãºteis (variÃ¡veis de ambiente)

| Nome            | Serve paraâ€¦                        | PadrÃ£o                     |
| --------------- | ---------------------------------- | -------------------------- |
| `CONFIG_FILE`   | JSON com seletores                 | *(vazio)*                  |
| `HTML_FALLBACK` | Caminho do HTML offline            | `suape_mapa_empresas.html` |
| `CHROMEDRIVER`  | Caminho do ChromeDriver (Selenium) | `chromedriver`             |
| `SELENIUM_PORT` | Porta onde o driver escuta         | `9515`                     |

VocÃª define assim (Linux/Mac):

```bash
export CONFIG_FILE=site_config.json
```

No Windows (PowerShell):

```ps1
setx CONFIG_FILE site_config.json
```

---

## 7. Quero entender o cÃ³digo (resumido)

```text
main()                # ponto de entrada
 â”œâ”€ tenta Selenium    # se disponÃ­vel
 â”œâ”€ tenta Requests    # se a pÃ¡gina Ã© simples
 â””â”€ tenta Offline     # se nÃ£o houver Internet
      â†³ _parse_empresas_from_soup()  # extrai campos e monta GeoJSON
```

Se uma estratÃ©gia falhar, ele tenta a prÃ³xima â€” por isso quase sempre funciona.

---

## 8. PrÃ³ximos passos (ideias)

* Exportar tambÃ©m em **CSV**.
* Criar menu de linha de comando (`python -m scraper --help`).
* Rodar todo dia com **GitHub Actions** para pegar dados novos.
* Fazer um **dashboard** no Streamlit mostrando o mapa.

---

## 9. DÃºvidas ou problemas?

Abra uma **issue** aqui no GitHub descrevendo:

* Link da pÃ¡gina que quer raspar;
* O que tentou fazer;
* Mensagens de erro (se houver).

Fico feliz em ajudar! ðŸ˜‰

---

> Desenvolvido durante o Desafio SUAPE 2025 e simplificado para qualquer pessoa que queira raspar listas da web.
