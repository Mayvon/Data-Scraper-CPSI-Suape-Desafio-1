# SUAPEÂ 'DataÂ Scraper'Â eÂ 'WebÂ Harvester'

> **Origem:** DesafioÂ SUAPE â€“ InovaÃ§Ã£o AbertaÂ 2025
> **Escopo original:** **DesafioÂ #1** â€“ *Sistema de EvasÃ£o de Pessoas*
> **Scriptâ€‘base:** `extracao.py`
> **Autor:** MayvonÂ Alves
> **LicenÃ§a:** MIT

Este repositÃ³rio nasceu como **parte especÃ­fica da soluÃ§Ã£o que apresentei para o DesafioÂ SUAPEÂ #1 na Chamada de InovaÃ§Ã£o Aberta via CPSI 2025**. No contexto da chamada pÃºblica, o **Ãºnico objetivo** do script foi **contar quantas empresas existem dentro do Complexo Industrial PortuÃ¡rio de SUAPE** e gerar um arquivo GeoJSON com seus pontos. Esse inventÃ¡rio numÃ©rico subsidiou anÃ¡lises posteriores do desafio, mas **nenhuma lÃ³gica de evacuaÃ§Ã£o ou roteamento** estÃ¡ aqui â€“ sÃ³ a **raspagem e georreferenciamento**.

Depois do programa, evoluÃ­ o cÃ³digo para servir como **ferramenta genÃ©rica de *scraping***: basta parametrizar seletores ou plugar outro *parser* para coletar dados tabulares de qualquer pÃ¡gina que apresente blocos repetitivos de HTML.

---

## Ãndice

1. [Contexto do Desafio](#contexto-do-desafio)
2. [VisÃ£o Geral do CÃ³digo](#visÃ£o-geral-do-cÃ³digo)
3. [Adaptando para Outros Sites](#adaptando-para-outros-sites)
4. [DependÃªncias](#dependÃªncias)
5. [Como Executar](#como-executar)
6. [Estrutura de SaÃ­da](#estrutura-de-saÃ­da)
7. [Design Decisions](#design-decisions)
8. [PrÃ³ximos Passos](#prÃ³ximos-passos)
9. [Contribuindo](#contribuindo)
10. [ReferÃªncias](#referÃªncias)

---

## Contexto do Desafio

O **Estudo TÃ©cnico Preliminar (ETP)** do DesafioÂ #1 (em `docs/Desafio_1.pdf`) pedia, como primeira etapa, um *levamento fidedigno* das empresas instaladas no perÃ­metro oficial do porto.â€¯Com esse nÃºmero em mÃ£os, a equipe de inovaÃ§Ã£o poderia:

1. **Dimensionar a populaÃ§Ã£o potencialmente exposta** em cenÃ¡rios de risco;
2. **Planejar testes de campo representativos** com base no total de stakeholders;
3. **Definir custos** de futuras integraÃ§Ãµes (ex.: licenÃ§as por empresa).

> ğŸ” **Importante:** O *crawler* nÃ£o contÃ©m qualquer lÃ³gica de avaliaÃ§Ã£o de risco ou evacuaÃ§Ã£o. Ele **apenas** gera a lista de empresas e respectiva contagem.

---

## VisÃ£o Geral do CÃ³digo

| Camada                                         | DescriÃ§Ã£o                         | DependÃªncias                 |
| ---------------------------------------------- | --------------------------------- | ---------------------------- |
|  **EstratÃ©giaÂ 1 â€“ Selenium**                 | Renderiza pÃ¡ginas JSâ€‘heavy.       | `selenium`, `chromedriver`   |
|  **EstratÃ©giaÂ 2 â€“ RequestsÂ +Â BeautifulSoup** | *Scraping* de HTML estÃ¡tico.      | `requests`, `beautifulsoup4` |
|  **EstratÃ©giaÂ 3 â€“ Offline**                  | Parseia um HTML salvo localmente. | Nenhuma                      |

O script percorre as estratÃ©gias **nesta ordem** e para na primeira que obtiver dados vÃ¡lidos, salvando um **GeoJSON** (EPSG:4326) com os pontos.

### Fluxo resumido

```
main()   â†’   estratÃ©gia (1|2|3)   â†’   _parse_empresas_from_soup()   â†’   GeoJSON
```

---

## Adaptando para Outros Sites

O projeto agora Ã© **configâ€‘driven**:

### 1. Defina seletores em JSON

Crie `site_config.json` com os seletores/atributos que descrevem cada campo:

```json
{
  "empresa_block": "div.card",        // bloco repetitivo
  "nome": ".card-title",             // seletor para o nome
  "atividade": ".card-atividade",     // seletor para a atividade
  "polo": ".tag-polo",               // seletor para o polo (opcional)
  "lat": "data-lat",                 // atributo ou seletor para latitude
  "lng": "data-lng"                  // atributo ou seletor para longitude
}
```

### 2. Execute com a configuraÃ§Ã£o

```bash
CONFIG_FILE=site_config.json python extracao.py --url "https://meu-site.com/lista"
```

### 3. Precisa de algo mais complexo?

Implemente um *parser* em `parsers/meu_site.py` com a assinatura:

```python
def parse(html: str) -> list[dict]:
    ...  # devolva features GeoJSON
```

O script detecta o mÃ³dulo automaticamente via `--parser meu_site`.

---

## DependÃªncias

* PythonÂ â‰¥Â 3.9
* `beautifulsoup4`
* `requests`
* `selenium` (opcional)

```bash
pip install -r requirements.txt
```

---

## Como Executar

```bash
# 1Â â€“ clone
git clone https://github.com/seu-usuario/suape-data-scraper.git
cd suape-data-scraper

# 2Â â€“ opÃ§Ãµes (exemplos)
export CONFIG_FILE="site_config.json"
export HTML_FALLBACK="backup.html"
export CHROMEDRIVER="/usr/bin/chromedriver"

# 3Â â€“ run!
python extracao.py --url "https://site-alvo.com"
```

| VariÃ¡vel        | Default                    | DescriÃ§Ã£o                    |
| --------------- | -------------------------- | ---------------------------- |
| `CONFIG_FILE`   | *(vazio)*                  | JSON com seletores/atributos |
| `HTML_FALLBACK` | `suape_mapa_empresas.html` | HTML offline                 |
| `CHROMEDRIVER`  | `chromedriver`             | WebDriver                    |
| `SELENIUM_PORT` | `9515`                     | Porta Selenium               |

---

## Estrutura de SaÃ­da

`*.geojson` no padrÃ£o `FeatureCollection`, contendo:

* `geometry` â€“ PointÂ (lon,Â lat)
* `properties.Nome`
* `properties.Atividade`
* `properties.Polo` (opcional)

Pronto para **QGIS**, **GeoPandas**, **Kepler.gl**, **MapboxÂ GLÂ JS** etc.

---

## Design Decisions

1. **Portabilidade first** â€“ roda atÃ© em Pyodide sem `ssl`.
2. **Failâ€‘safe** â€“ trÃªs estratÃ©gias minimizam falhas.
3. **Configâ€‘driven** â€“ seletores externos ao cÃ³digo.
4. **GeoJSON** â€“ formato aberto e versionÃ¡vel.

---

## PrÃ³ximos Passos

* CLI oficial (`python -m scraper --help`).
* SaÃ­da em CSV e Parquet.
* GitHubÂ Actions para rodar *scraping* agendado.
* Exemplo de dashboard Streamlit.

---

## Contribuindo

PullÂ Requests sÃ£o bemâ€‘vindos! Antes, abra uma *issue* contendo:

* URL alvo;
* Campos desejados;
* Especificidades do HTML.

---

## ReferÃªncias

* **ETPÂ â€“Â DesafioÂ #1** (`docs/Desafio_1.pdf`).
* Site oficial do Complexo deÂ SUAPE.
* Beautiful Soup Documentation.

---

> Feito para o DesafioÂ SUAPE, Chamada de InovaÃ§Ã£o Aberta 2025 â€“ e evoluÃ­do para qualquer pessoa que precise de um *scraper* plugâ€‘andâ€‘play.
